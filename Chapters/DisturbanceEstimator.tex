This section concerns the estimator used for estimation of the consumer demand flows.

\subsection{Estimator Objective}
The LQR controller relies on knowledge of the current  consumer demand flows. The consumption flow is not measured directly but can be calculated from the combined measurements of tank flow $d_\tau$ and pump flows $d_p$. If no leakage is present all nodal demands sum to zero and thus $d_c = d_p + d_\tau$.  Noise will be present on all flow sensor measurements which will affect LQR controller performance. Thus it is favorable to estimate the actual consumer flow.

\subsection{Estimator model}
An initial solution for estimating the consumer flow is to utilize the full fast dynamic system matrix. This requires the ability to accurately model the full system which has been done in this case but in practice this is not feasible. Therefore another solution is explored.

If prior knowledge of the behavior of the process is available then this can be taken advantage of. A new state space model describing the behavior of the process can be made. The consumption can be approximated with a harmonic series with increasing precision with higher harmonic order. From the consumption pattern data reviewed in \cref{sec:kalman_imp} it becomes apparent that an up to forth order approximation seems appropriate. This approximation is seen in \cref{eq:consump_pattern_actual}.

\begin{equation} \label{eq:consump_pattern_actual}
	d_{c\_approximation} = K + k_1 cos(\omega t) + k_2 cos(\frac{\omega}{2} t) + k_3 cos(\frac{\omega}{4} t) + k_4 cos(\frac{\omega}{8} t)
\end{equation}

The development and utilization of the pattern model is sufficiently described for a second order harmonic example. Thus only a model of this order is further examined here. Such a model can be seen in \cref{eq:consump_pattern}. 

\begin{equation} \label{eq:consump_pattern}
	d_{c_estimate} = K + k_1 cos(\omega t) + k_2 cos(\frac{\omega}{2} t)
\end{equation}



This process model output should be achievable as a combination of states. As such the following states are chosen:

\begin{equation} \label{eq:consump_x}
	x_{desired} =  \begin{bmatrix}
		x_1 \\
		x_2 \\
		x_3
	\end{bmatrix}
	=
	\begin{bmatrix}
		K \\
		k_1 cos(\omega t) \\
		k_2 cos(\frac{\omega}{2} t)
	\end{bmatrix}
\end{equation}

As with any state-space system these states need to be expressed in terms of their derivatives ($\dot x = Ax$) which are:

\begin{equation} \label{eq:consump_x_deriv_s}
	\dot x_{desired}  = \begin{bmatrix}
		\dot x_1 \\
		\dot x_2 \\
		\dot x_3
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		- \omega k_1 sin(\omega t) \\
		- \frac{\omega}{2} k_2 sin(\frac{\omega}{2} t)
	\end{bmatrix}
\end{equation}

No linear combination of our current states can yield this derivative. Thus we need to expand our states such that this can be achieved. Our state vector expands to:

\begin{equation} \label{eq:consump_x_l}
		x =  \begin{bmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4 \\
			x_5
		\end{bmatrix}
		=
		 \begin{bmatrix}
		K \\
		k_1 cos(\omega t) \\
		k_2 cos(\frac{\omega}{2} t) \\
		k_1 sin(\omega t) \\
		k_2 sin(\frac{\omega}{2}) 
	\end{bmatrix}
\end{equation}

Which has the state derivative:

\begin{equation} \label{eq:consump_x_deriv_l}
	\dot x =  \begin{bmatrix}
		\dot x_1 \\
		\dot x_2 \\
		\dot x_3 \\
		\dot x_4 \\
		\dot x_5
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		- \omega k_1 sin(\omega t) \\
		- \frac{\omega}{2} k_2 sin(\frac{\omega}{2} t) \\
		\omega k_1 cos (\omega t) \\
		\frac{\omega}{2} k_2 cos (\frac{\omega}{2} t)
	\end{bmatrix}
\end{equation}


The system matrix then needs to be:

\begin{equation} \label{eq:consump_A}
	A = \begin{bmatrix}
		0 & 0 				& 0					& 0 				& 0 \\
		0 & 0 				& 0					& -\omega 	& 0 \\
		0 & 0				& 0					& 0 				& - \omega/2 \\
		0 & \omega	& 0						& 0 				& 0 \\
		0 & 0				& \omega/2 	& 0					& 0
	\end{bmatrix}
\end{equation}

For $y = Cx$ to be equal to \cref{eq:consump_pattern} the C-matrix becomes: 

\begin{equation}
	C = \begin{bmatrix} 1 & 1 & 1 & 0 & 0 \end{bmatrix}
\end{equation}

If one wishes have a signal model of greater harmonic orders, a concatenation of e.g. the first order harmonic signal model, like seen in \cref{eq:first_order_signal_model}, can be performed for an arbitrary amount of harmonics. 

\begin{equation}\label{eq:first_order_signal_model}
	\begin{gathered}
		\dot{x} = A_\delta x =  \begin{bmatrix}0 & 0 & 0 \\ 0 & 0 & -\omega \\ 0 & \omega & 0	\end{bmatrix}x \\
		y = C_\delta x = \begin{bmatrix} 1 & 1 & 0 \end{bmatrix} x
	\end{gathered}
\end{equation}

Such a concatenation would for a second order approximation look like such:

\begin{equation}\label{eq:DisturbanceVectorCase}
	\begin{gathered}
		\begin{bmatrix} K \\ \dot{x}_1 \\ \dot{x}_2 \end{bmatrix} = \begin{bmatrix} A_{\delta_1} & 0 \\ 0 & \bar{A}_{\delta_2} \end{bmatrix} \begin{bmatrix} K \\ x_1 \\ x_2 \end{bmatrix},
		\quad y = C_\delta \begin{bmatrix} K \\ x_1 \\ x_2 \end{bmatrix} \\
		x_1, x_2 \in \mathbb{R}^{2\times1} \\
		A_{\delta_1} \in \mathbb{R}^{3\times3}, \quad \forall i \neq 1: \ \bar{A}_{\delta_i} \in \mathbb{R}^{2\times2} \\
		C_\delta \in \mathbb{R}^{1\times2n+1}
	\end{gathered}
\end{equation}

\subsection{Estimator type}
Several estimator options are available, including the classical Luenberg observer. Although the Kalman filter is the desired estimator if it is assumed that:

\begin{enumerate}
	\item the consumer flow disturbance can be modeled as a dynamic system excited by white zero-mean uncorrelated gaussian noise
	\item the flow measurement noise can be considered white zero-mean uncorrelated gaussian noise
\end{enumerate}

It is the optimal affine estimator given some assumptions about the precision of the model of the process and the knowledge of the process and measurement noise, which is further described in \cref{sec:the_kalman_filter}.





\subsection{The Kalman Filter} \label{sec:the_kalman_filter}
The Kalman filter is an optimal estimator, where the optimality criterion is to reduce the Mean Squared Error (MSE) of the residuals.
It estimates unknown processes by a combination of a system model and a measurement that is related to the unknown processes. 

The Kalman filter recursively finds the optimal \textit{Kalman Gain} for the given system to minimize the residual MSE. Finding the optimal gain relies on assuming that the model and measurement noise are uncorrelated white noise processes with known covariances. 

The performance of the Kalman filter relies heavily on the guessing the covariance of measurements and model noise. This means in practice that an online tuning of covariances is desirable to avoid bad guesses of the covariances \cite{Doraiswami2014} p. 232.

\subsubsection{Mathematical formulation of the Kalman filter in state space}
The Kalman Filter described in this report will based on a discrete time state space representation: 

\begin{align}
	&\textbf{x}(k+1) = \textbf{A}\textbf{x}(k) + \textbf{B}\textbf{u}(k) + \textbf{w}(k)  \label{eq:KalmanSystemEquations} \\
	&\textbf{y}(k) = \textbf{C}\textbf{x}(k)+\textbf{v}(k) 
\end{align}
where 
\begin{align*}
	&\text{$\textbf{x}(k)$ is the state vector at time index k,					}	\\[-1em]
	&\text{$\textbf{u}(k)$ is the input at time index k, 						}	\\[-1em]
	&\text{$\textbf{w}(k) \sim \mathcal{N}(0, Q)$ is the model noise,			}	\\[-1em]
	&\text{$\textbf{v}(k) \sim \mathcal{N}(0, R)$ is the measurement noise,		}	\\[-1em]
	&\text{$\textbf{y}(k)$ is the observable output, 							}	\\[-1em]
	&\text{\textbf{A} is the state transition matrix,							}	\\[-1em]
	&\text{\textbf{B} is the control-input matrix,								}	\\[-1em]
	&\text{\textbf{C} is the observation matrix. 								}	\\[-1em]
\end{align*}

To simplify the presentation for the readers of this report the Kalman equations will be presented in the form of 9 equations, after which they can be combined to yield the traditional and more dense form.
The Kalman equations will further be divided into two phases: the prediction stage and the update stage. 

\textbf{Prediction stage}
\begin{align}
	&\hat{\textbf{x}}	(k|k-1) = \textbf{A} 	\hat{\textbf{x}}(k-1|k-1)  				\label{eq:Kalman_pred_state} 	\\
	&\hat{\textbf{y}}	(k|k-1) = \textbf{C}	\hat{\textbf{x}}(k|k-1)										\label{eq:Kalman_pred_output} 	\\
	&\textbf{P}			(k|k-1) = \textbf{A}	\textbf{P}(k-1|k-1)\textbf{A}^T+\textbf{Q} 								\label{eq:Kalman_pred_cov} 		
\end{align}

where 
\begin{align*}
	&\text{$\hat{\textbf{x}}	(k|k-1)$ 	is the state prediction 			estimate at time index $k$ 		given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\hat{\textbf{x}}	(k-1|k-1)$ 	is the state 						estimate at time index $k-1$ 	given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\hat{\textbf{y}}	(k|k-1)$ 	is the output prediction 			estimate at time index $k$ 		given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\textbf{P}			(k|k-1)$ 	is the predicted estimate  covariance matrix at time index $k$ 		given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\textbf{P}			(k-1|k-1)$ 	is the updated estimate    covariance matrix at time index $k-1$ 	given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\textbf{Q}$						is the model/process noise covariance matrix														}	\\[-1em]
\end{align*}

All three prediction equations predicts the a priori $k$ (next) estimate based on the $k-1$ previous samples and the model knowledge.\\
\textbf{Update stage}

\begin{align}
	&\textbf{e}			(k) 		= \textbf{y}(k) - \hat{\textbf{y}}(k|k-1)							\label{eq:Kalman_upd_inno}			\\
	&\textbf{S}			(k) 		= \textbf{C}\textbf{P}(k|k-1)\textbf{C}^T + \textbf{R}				\label{eq:Kalman_upd_inno_cov}		\\
	&\textbf{K}			(k) 		= \textbf{P}(k|k-1)\textbf{C}^T\textbf{S}^{-1}(k)					\label{eq:Kalman_upd_kalman_gain}	\\
	&\hat{\textbf{x}}	(k|k) 		= \hat{\textbf{x}}(k|k-1) + \textbf{K}(k)\textbf{e}(k) 				\label{eq:Kalman_upd_est_state}		\\
	&\textbf{P}			(k|k) 		= (\textbf{I} - \textbf{K}(k)\textbf{C})\textbf{P}(k|k-1)			\label{eq:Kalman_upd_est_cov}
\end{align}

where 

\begin{align*}
	&\text{$\textbf{e}		(k)$ 		is the innovation term for time index 					$ k $													}	\\[-1em]
	&\text{$\textbf{y}		(k)$ 		is the observed output at time index 					$ k $													}	\\[-1em]
	&\text{$\textbf{S}		(k)$ 		is the innovation covariance for time index 			$ k $											}	\\[-1em]
	&\text{$\textbf{R}$ 				is the observation noise covariance																}	\\[-1em]
	&\text{$\textbf{K}		(k)$ 		is the Kalman gain for time index 						$ k $													}	\\[-1em]
	&\text{$\hat{\textbf{x}}(k|k)$ 	is the estimate of the state vector at time index 			$ k $ given all 	$ k$ samples	}	\\[-1em]
	&\text{$\textbf{P}		(k|k)$ 	is the updated estimate covariance matrix at time index 	$ k $ given all 	$ k$ samples		}	\\[-1em]
	&\text{$\textbf{I}$ 				is the identity matrix																				}	\\[-1em]			
\end{align*}

All the equations in the update stage estimates the posteriori estimate of the $ k $ sample based on all $ k $ samples and the model knowledge. A few more remarks to give some intuitions to what the equations actually represent. The estimate covariance and innovation covariance can respectively be represented as 

\begin{align}
	&\textbf{P}(k|k) 	= \text{cov}(\textbf{x}(k)-	\hat{\textbf{x}}(k|k))	\\
	&\textbf{P}(k|k-1) 	= \text{cov}(\textbf{x}(k)-	\hat{\textbf{x}}(k|k-1)) 		\\
	&\textbf{S}(k) 		= \text{cov}(\textbf{e}(k)) 
\end{align}

\textbf{Compact form}\\
Now collecting some of the above equations gives the more compact form of the Kalman equations.
Eq.'s \ref{eq:Kalman_pred_state} and \ref{eq:Kalman_pred_cov} remains in the same form, but omitting $ \textbf{Bu}(k-1) $ yields \cref{eq:Kalman_pred_state_compact} and \cref{eq:Kalman_pred_cov_compact}. Eq.'s \ref{eq:Kalman_upd_inno_cov}, \ref{eq:Kalman_upd_kalman_gain} are combined yielding \cref{eq:Kalman_upd_kalman_gain_compact}. Eq. \ref{eq:Kalman_pred_output} is combined with \cref{eq:Kalman_upd_inno} and \cref{eq:Kalman_upd_est_state} yielding \cref{eq:Kalman_upd_est_state_compact}, while \cref{eq:Kalman_upd_est_cov} remains the same. All these combined presents the kalman equations as in \cite{Bozic1994}\\
\textbf{Prediction}

\begin{align}
	&\hat{\textbf{x}}	(k|k-1) = \textbf{A} \hat{\textbf{x}}	(k-1|k-1) 		\label{eq:Kalman_pred_state_compact} 	\\
	&\textbf{P}			(k|k-1) = \textbf{A}\textbf{P}			(k-1|k-1)\textbf{A}^T+\textbf{Q} 				\label{eq:Kalman_pred_cov_compact} 		
\end{align}

\textbf{Update}
\begin{align}
	&\textbf{K}			(k) 		= \textbf{P}				(k|k-1)\textbf{C}^T(\textbf{C}\textbf{P}	(k|k-1)	\textbf{C}^T + \textbf{R})^{-1}										\label{eq:Kalman_upd_kalman_gain_compact} \\
	&\hat{\textbf{x}}	(k|k) 	= \hat{\textbf{x}}			(k|k-1) + \textbf{K}						(k)	(\textbf{y}		(k) - \textbf{C}\hat{\textbf{x}}		(k|k-1)) 	\label{eq:Kalman_upd_est_state_compact} \\
	&\textbf{P}			(k|k) 	= (\textbf{I} - \textbf{K}	(k)\textbf{C})\textbf{P}					(k|k-1)																		\label{eq:Kalman_upd_est_cov_compact}
\end{align}


\subsubsection{Steady-state Kalman filter}
If we assume that our system is time invariant, which we have done so far, the estimate Covariance and Kalman gain can be calculated analytically as they don't depend on measurements and because A and C are constant. As such, the Kalman filter itself becomes a LTI filter. The steady state estimate covariance prediction can be found by solving its Algebraic Riccati equation. We will attempt to derive this.\\

%\begin{align}
%	&\textbf{P} = \textbf{APA}^T - \textbf{APC}^T(\textbf{CPC}^T+\textbf{R})^{-1} \textbf{CPA}^T+ \textbf{Q} \\
%	&\textbf{K} = \textbf{APC}^T(\textbf{CPC}^T+\textbf{R})^{-1}
%\end{align}

In steady-state where $k \rightarrow \infty$ we can assume that $\textbf{P}(k|k) = \textbf{P}(k-1|k-1) $. 

The predicted MSE, the MSE and the Kalman Gain approach a constant value:

\begin{equation}
	\textbf{P}(k|k-1) =  \textbf{P}_p(\infty), \hspace{3mm} \textbf{P}(k|k) = \textbf{P}(k-1|k-1) = \textbf{P}(\infty),  \hspace{3mm} \textbf{K}(k) = \textbf{K}(\infty)
\end{equation}

Equations \cref{eq:Kalman_upd_kalman_gain_compact}, \cref{eq:Kalman_upd_est_state_compact} and \cref{eq:Kalman_upd_est_cov_compact} are rewritten with their steady state counterparts:
\begin{align}
	& \textbf{K}(\infty) = \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p	(\infty) \textbf{C}^T + \textbf{R})^{-1} \label{eq:ss_kalman_kalman_gain} \\
	& \textbf{P}(\infty) = (\textbf{I} - \textbf{K}(\infty) \textbf{C}) \textbf{P}_p(\infty) \label{eq:ss_kalman_mse} \\
	& \textbf{P}_p(\infty) = \textbf{A}\textbf{P}(\infty)\textbf{A}^T + \textbf{Q} \label{eq:ss_kalman_mse_pred}
\end{align}

We go on the hunt for the damened Riccati equation of $\textbf{P}_p(\infty)$. We substitute RHS of \cref{eq:ss_kalman_mse} into \cref{eq:ss_kalman_mse_pred}.

\begin{equation}
	\begin{split}\label{eq:ss_kalman_udledning1}
		\textbf{P}_p(\infty) 	& = \textbf{A}\textbf{P}(\infty)A^T + Q \\
		& = \textbf{A} (\textbf{I}-\textbf{K}(\infty)\textbf{C}) \textbf{P}_p(\infty) \textbf{A}^T + \textbf{Q}\\
		& = \textbf{A} \textbf{P}_p(\infty) \textbf{A}^T - \textbf{A} \textbf{K}(\infty) \textbf{C} \textbf{P}_p(\infty)  \textbf{A}^T + \textbf{Q}\\
		\end{split}
\end{equation}
We now substitute RHS of \cref{eq:ss_kalman_kalman_gain} in for $\textbf{K}(\infty)$ in \cref{eq:ss_kalman_udledning1} and factorize respectively $\textbf{A}$ and $ \textbf{A}^T $.
\begin{equation}
	\begin{split}\label{eq:ss_kalman_udledning2}
		\textbf{P}_p(\infty) & = \textbf{A} \textbf{P}_p(\infty) \textbf{A}^T - \textbf{A} \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p(\infty) \textbf{C}^T + \textbf{R})^{-1} \textbf{C} \textbf{P}_p(\infty) \textbf{A}^T + \textbf{Q}\\
		& = \textbf{A} (\textbf{P}_p(\infty) - \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p(\infty) \textbf{C}^T + \textbf{R})^{-1} \textbf{C} \textbf{P}_p(\infty)) \textbf{A}^T + \textbf{Q}\\
	\end{split}
\end{equation}
And let The Matrix Inversion \cref{eq:ss_kalman_MatrixInversionLemma} lemma be used to further simplify whats left in the parenthesis of RHS of \cref{eq:ss_kalman_udledning2} by letting
\begin{equation}
		\textbf{A}^{-1} = \textbf{P}_p(\infty), \textbf{B} = \textbf{C}^T, \textbf{C}^{-1} = \textbf{R}, \textbf{D} = \textbf{C} \label{eq:ss_kalman_udledning3}
\end{equation}
enables 
\begin{align}
		(\textbf{A}+\textbf{BCD})^{-1} &= \textbf{A}^{-1} - \textbf{A}^{-1}\textbf{B} (\textbf{D}\textbf{A}^{-1}\textbf{B}+\textbf{C}^{-1})^{-1}\textbf{D}\textbf{A}^{-1} \\ \label{eq:ss_kalman_MatrixInversionLemma}
		(\textbf{P}_p(\infty)^{-1} + \textbf{C}^T \textbf{R}^{-1} \textbf{C})^{-1} &= (\textbf{P}_p(\infty) - \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p(\infty) \textbf{C}^T + \textbf{R})^{-1} \textbf{C} \textbf{P}_p(\infty))
\end{align}
yielding the desired result: The Algebraic Riccati equation in a compact form, \cref{eq:ss_kalman_udledning4}.
\begin{equation}
	\begin{split}\label{eq:ss_kalman_udledning4}
		&\textbf{P}_p(\infty) = \textbf{A} (\textbf{P}_p(\infty)^{-1} + \textbf{C}^T \textbf{R}^{-1} \textbf{C})^{-1} \textbf{A}^T + \textbf{Q}\\
	\end{split}
\end{equation}

\cref{eq:ss_kalman_udledning4} can be solved for $\textbf{P}_p(\infty)$ numerically and substitued into \cref{eq:ss_kalman_kalman_gain} yielding \textit{The Steady State Kalman Gain} $ \textbf{K}(\infty) $ which is then substituted into \cref{eq:ss_kalman_mse} to obtain \textit{Steady-State Estimate Covariance} $ \textbf{P}(\infty) $.
\begin{align}
	& \textbf{K}(\infty) = \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p	(\infty) \textbf{C}^T + \textbf{R})^{-1} \label{eq:ss_kalman_kalman_gain1} \\
	& \textbf{P}(\infty) = (\textbf{I} - \textbf{K}(\infty) \textbf{C}) \textbf{P}_p(\infty) \label{eq:ss_kalman_mse1} 
\end{align}

An attempt to identify the Algebraic Riccati equation for the Steady-State Estimate Covariance matrix was also attempted:


\begin{equation}
	\begin{split}\label{eq:ss_kalman_udledning5}
	 	\textbf{P}(\infty) 	& = (\textbf{I} - \textbf{K}(\infty) \textbf{C}) \textbf{P}_p(\infty) \\ %\label{eq:ss_kalman_mse2} 
		& = \textbf{P}_p(\infty) - \textbf{K}(\infty) \textbf{C}\textbf{P}_p(\infty) \\
		\text{substituting RHS of \cref{eq:ss_kalman_kalman_gain1}} \\
		& = \textbf{P}_p(\infty) - \textbf{P}_p(\infty) \textbf{C}^T [\textbf{C} \textbf{P}_p	(\infty) \textbf{C}^T + \textbf{R}]^{-1}\textbf{C}\textbf{P}_p(\infty) \\
		\text{Again applying the Matrix Inversion lemma by letting} \\
		\textbf{A}^{-1} = \textbf{P}_p(\infty), \textbf{B} = \textbf{C}^T, &\textbf{C}^{-1} = \textbf{R}, \textbf{D} = \textbf{C} \\
		\text{Yielding} \\
		\textbf{P}(\infty) 	& = [\textbf{P}_p(\infty) + \textbf{C}^T\textbf{R}^{-1}\textbf{C}]^{-1} \\
		\text{Substituting RHS of \cref{eq:ss_kalman_mse_pred} yields}  \\
	\end{split}
\end{equation}
\begin{align}
		\textbf{P}(\infty) & = [(\textbf{A}\textbf{P}(\infty)\textbf{A}^T + \textbf{Q}) + \textbf{C}^T\textbf{R}^{-1}\textbf{C}]^{-1}  \label{eq:ss_kalman_udledning6}
\end{align}


Which indeed is the Algebraic Riccati equation for the Estimate Covariance matrix $\textbf{P}(\infty)$\\
\\

\textbf{First question}\\

CARSTEN - HERE COMES THE QUESTION.. HOW DO WE SHOW THAT RESPECTIVELY \cref{eq:ss_kalman_udledning6} AND \cref{eq:ss_kalman_udledning4} ARE RICCATI EQUATIONS, I.E. SOMETHING LIKE
\begin{align}
	&0 = aP^2 + bP + c 
%	&P = A^TPA - (APC^T)(R+CPC^T)^{-1}(CPA^T)+Q 
\end{align}
IS IT MARELY TO ACKNOWLEDGE THAT THE DIFFERENCE BETWEEN HIGHEST ORDER AND LOWEST ORDER OF P IS 2?

\textbf{second question}\\
In many sources (mostly slides and Wikipedia), the Riccati equation for Estimate Covariance is 
\begin{align}
	&\textbf{P} = \textbf{APA}^T - (\textbf{APC}^T)(\textbf{R}+\textbf{CPC}^T)^{-1}(\textbf{CPA}^T)+\textbf{Q}  \\
	\text{But the closest we can get is } \\
	&\textbf{P} = \textbf{APA}^T - (\textbf{APA}^T+ \textbf{Q})\textbf{C}^T[\textbf{C}(\textbf{APA}^T+\textbf{Q})\textbf{C}^T]^{-1}\textbf{C}(\textbf{APA}^T + \textbf{Q}) + \textbf{ Q}
\end{align}
We suggest it doesn't matter what form it comes in? If it does, we need help to arrive at the form above.




\subsubsection{Implementation of Kalman filter} \label{sec:kalman_imp}
For our case, the disturbance model has $ \textbf{u}(k)=0 $ meaning that our implementation can disregard the $ \textbf{B}\textbf{u}(k) $ in \cref{eq:Kalman_pred_state_compact}.


The steady state Covariance matrix P and kalman Gain is ... (asymtotic kalman gain and P)
Constant Q and R, there will be an asymptotic solution - Riccati equations. 

\subsubsection{Assumptions about noise}
Describe covariance matrices Q and R\\





\subsubsection{Kalman simulation}
Real water consumption data should be analyzed in order to justify approximating the consumption pattern as a harmonic series. A historical data set showing typical daily water consumption during a three month period is attained. Two arbitrary days from said data set is seen in \cref{fig:Consumptionpattern}. Note that the unit of consumption is unknown, and we will normalize this data to fit the expected maximum flow in the laboratory.  Using frequency analysis we construct a model containing the four most influential sinusoidal components, assuming these will contain the general tendency of the pattern.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{Pictures/ConsumptionPattern.pdf}
	
	\caption{Consumption pattern over two days}
	\label{fig:Consumptionpattern}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{Pictures/FFT.pdf}
	
	\caption{FFT results on consumption pattern}
	\label{fig:FFT}
\end{figure}

FFT analysis yield amplitude and phase plots as seen in \cref{fig:FFT}.The amplitude plot clearly shows a bias and the most influential components. To construct the correct signal we also need to identify the phase shift of each components. These can be extracted from the phase plot, by identifying which indices correspond to the most influential frequencies.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{Pictures/Comparisson.pdf}
	
	\caption{Comparison of raw historical data, and model}
	\label{fig:Comparison}
\end{figure}

The resulting model is compared to the raw historical data in \cref{fig:Comparison}, showing moderate coincidence. Estimate accuracy in a specific application will benefit from more thorough design of the model, possibly including additional harmonics and consumption data from the local area. Therefore, we are satisfied with the model as a general example of a consumer pattern, without requirements on validity or application. A minor correction is made based on visual inspection of the plot, resulting in a slight timeshift of the entire model to match peaks of the fundamental frequency. We justify this correction based on seeing similar shift if a few extra harmonics are included.


The Kalman filters ability to track the model is verified in simulations, and simultaneously allow us to make initial guesses on covariance matrices. We have established that a model may accurately represent consumer patterns and therefore allow the estimator to place greater trust in the model than the measurements. This also benefits our ability to detect a leakage, as the error between estimate and measurement becomes more prominent when the model is the primary contributor in the filter. This leads to somewhat arbitrary initial covariance guesses where the consumer model is trusted ten times more than the measurement.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{Pictures/Comparisson.pdf}
	
	\caption{Kalman comparison with noisy model and altered model}
	\label{fig:Comparison}
\end{figure}

A large white noise is applied to the signal, we assume this noise to be much greater than any practical occurrence, and therefore assume the filter will perform better than simulation results in any real application. The model from which measurements are drawn is also altered to simulate a discrepancy between Kalman model and real measurements. This error is introduced by increasing amplitude of the first two sinusoidal components by 20\%, and removing the third component. In theory this could correspond to a hot day increasing the general water consumption, where a large factory which create the third harmonic is out of operation.
These covariance values, allow the Kalman filter to track the model accurately even with error. However, this property is disadvantageous in terms of detecting leakage, as a deviation from the expected behavior given by the model, would be concealed as the filter tracks the deviation too well. This introduces a tradeoff where we wish to be able to accurately estimate the consumer pattern i.e. thrust measurements, such that minor changes in the pattern are tracked, and still rely enough on the model making larger deviations stand out.

Unfinished comment; If leaks are assumed to appear rappidly, say over 10 seconds. Then ...