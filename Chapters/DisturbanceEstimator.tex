This section concerns the estimator used for estimation of the consumer demand flows.

\subsection{Estimator Objective}
The LQR controller relies on knowledge of the current  consumer demand flows. The consumption flow is not measured directly but can be calculated from the combined measurements of tank flow $d_\tau$ and pump flows $d_p$. If no leakage is present all nodal demands sum to zero and thus $d_c = d_p + d_\tau$.  Noise will be present on all flow sensor measurements which will affect LQR controller performance. Thus it is favorable to estimate the actual consumer flow.

\subsection{Estimator model}
An initial solution for estimating the consumer flow is to utilize the full fast dynamic system matrix. This requires the ability to accurately model the full system which has been done in this case but in practice this is not feasible. Therefore another solution is explored.

If prior knowledge of the behavior of the process is available then this can be taken advantage of. A new state space model describing the behavior of the process can be made. The consumption pattern behaves like a harmonic series (up to 4th order???) like seen in \cref{eq:consump_pattern}.

\begin{equation} \label{eq:consump_pattern}
	d_{c_estimate} = K + k_1 cos(\omega t) + k_2 cos(\frac{\omega}{2} t)
\end{equation}



This process model output should be achievable as a combination of states. As such the following states are chosen:

\begin{equation} \label{eq:consump_x}
	x_{desired} =  \begin{bmatrix}
		x_1 \\
		x_2 \\
		x_3
	\end{bmatrix}
	=
	\begin{bmatrix}
		K \\
		k_1 cos(\omega t) \\
		k_2 cos(\frac{\omega}{2} t)
	\end{bmatrix}
\end{equation}

As with any state-space system these states need to be expressed in terms of their derivatives ($\dot x = Ax$) which are:

\begin{equation} \label{eq:consump_x_deriv_s}
	\dot x_{desired}  = \begin{bmatrix}
		\dot x_1 \\
		\dot x_2 \\
		\dot x_3
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		- \omega k_1 sin(\omega t) \\
		- \frac{\omega}{2} k_2 sin(\frac{\omega}{2} t)
	\end{bmatrix}
\end{equation}

No linear combination of our current states can yield this derivative. Thus we need to expand our states such that this can be achieved. Our state vector expands to:

\begin{equation} \label{eq:consump_x_l}
		x =  \begin{bmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4 \\
			x_5
		\end{bmatrix}
		=
		 \begin{bmatrix}
		K \\
		k_1 cos(\omega t) \\
		k_2 cos(\frac{\omega}{2} t) \\
		k_1 sin(\omega t) \\
		k_2 sin(\frac{\omega}{2}) 
	\end{bmatrix}
\end{equation}

Which has the state derivative:

\begin{equation} \label{eq:consump_x_deriv_l}
	\dot x =  \begin{bmatrix}
		\dot x_1 \\
		\dot x_2 \\
		\dot x_3 \\
		\dot x_4 \\
		\dot x_5
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		- \omega k_1 sin(\omega t) \\
		- \frac{\omega}{2} k_2 sin(\frac{\omega}{2} t) \\
		\omega k_1 cos (\omega t) \\
		\frac{\omega}{2} k_2 cos (\frac{\omega}{2} t)
	\end{bmatrix}
\end{equation}


The system matrix then needs to be:

\begin{equation} \label{eq:consump_A}
	A = \begin{bmatrix}
		0 & 0 				& 0					& 0 				& 0 \\
		0 & 0 				& 0					& -\omega 	& 0 \\
		0 & 0				& 0					& 0 				& - \omega/2 \\
		0 & \omega	& 0						& 0 				& 0 \\
		0 & 0				& \omega/2 	& 0					& 0
	\end{bmatrix}
\end{equation}

For $y = Cx$ to be equal to \cref{eq:consump_pattern} the C-matrix becomes: 

\begin{equation}
	C = \begin{bmatrix} 1 & 1 & 1 & 0 & 0 \end{bmatrix}
\end{equation}

\subsection{Estimator type}
Several estimator options are available, including the classical Luenberg observer. Although the Kalman filter is the desired estimator if it is assumed that:

\begin{enumerate}
	\item the consumer flow disturbance can be modeled as a dynamic system excited by white zero-mean uncorrelated gaussian noise
	\item the flow measurement noise can be considered white zero-mean uncorrelated gaussian noise
\end{enumerate}

It is the optimal affine estimator given some assumptions about the precision of the model of the process and the knowledge of the process and measurement noise, which is further described in \cref{sec:the_kalman_filter}.





\clearpage \newpage
\subsection{The Kalman Filter} \label{sec:the_kalman_filter}
The Kalman filter is an optimal estimator, where the optimality criterion is to reduce the Mean Squared Error (MSE) of the residuals.
It estimates unknown processes by a combination of a system model and a measurement that is related to the unknown processes. 

The Kalman filter recursively finds the optimal \textit{Kalman Gain} for the given system to minimize the residual MSE. Finding the optimal gain relies on assuming that the model and measurement noise are uncorrelated white noise processes with known covariances. 

The performance of the Kalman filter relies heavily on the guessing the covariance of measurements and model noise. This means in practice that an online tuning of covariances is desirable to avoid bad guesses of the covariances \cite{Doraiswami2014} p. 232.

\subsubsection{Mathematical formulation of the Kalman filter in state space}
The Kalman Filter described in this report will based on a discrete time state space representation: 

\begin{align}
	&\textbf{x}(k+1) = \textbf{A}\textbf{x}(k) + \textbf{B}\textbf{u}(k) + \textbf{w}(k)  \label{eq:KalmanSystemEquations} \\
	&\textbf{y}(k) = \textbf{C}\textbf{x}(k)+\textbf{v}(k) 
\end{align}
where 
\begin{align*}
	&\text{$\textbf{x}(k)$ is the state vector at time index k,					}	\\[-1em]
	&\text{$\textbf{u}(k)$ is the input at time index k, 						}	\\[-1em]
	&\text{$\textbf{w}(k) \sim \mathcal{N}(0, Q)$ is the model noise,			}	\\[-1em]
	&\text{$\textbf{v}(k) \sim \mathcal{N}(0, R)$ is the measurement noise,		}	\\[-1em]
	&\text{$\textbf{y}(k)$ is the observable output, 							}	\\[-1em]
	&\text{\textbf{A} is the state transition matrix,							}	\\[-1em]
	&\text{\textbf{B} is the control-input matrix,								}	\\[-1em]
	&\text{\textbf{C} is the observation matrix. 								}	\\[-1em]
\end{align*}

To simplify the presentation for the readers of this report the Kalman equations will be presented in the form of 9 equations, after which they can be combined to yield the traditional and more dense form.
The Kalman equations will further be divided into two phases: the prediction stage and the update stage. 

\textbf{Prediction stage}
\begin{align}
	&\hat{\textbf{x}}	(k|k-1) = \textbf{A} 	\hat{\textbf{x}}(k-1|k-1)  				\label{eq:Kalman_pred_state} 	\\
	&\hat{\textbf{y}}	(k|k-1) = \textbf{C}	\hat{\textbf{x}}(k|k-1)										\label{eq:Kalman_pred_output} 	\\
	&\textbf{P}			(k|k-1) = \textbf{A}	\textbf{P}(k-1|k-1)\textbf{A}^T+\textbf{Q} 								\label{eq:Kalman_pred_cov} 		
\end{align}

where 
\begin{align*}
	&\text{$\hat{\textbf{x}}	(k|k-1)$ 	is the state prediction 			estimate at time index $k$ 		given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\hat{\textbf{x}}	(k-1|k-1)$ 	is the state 						estimate at time index $k-1$ 	given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\hat{\textbf{y}}	(k|k-1)$ 	is the output prediction 			estimate at time index $k$ 		given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\textbf{P}			(k|k-1)$ 	is the predicted estimate  covariance matrix at time index $k$ 		given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\textbf{P}			(k-1|k-1)$ 	is the updated estimate    covariance matrix at time index $k-1$ 	given all $ k-1 $ samples		}	\\[-1em]
	&\text{$\textbf{Q}$						is the model/process noise covariance matrix														}	\\[-1em]
\end{align*}

All three prediction equations predicts the a priori $k$ (next) estimate based on the $k-1$ previous samples and the model knowledge.\\
\textbf{Update stage}

\begin{align}
	&\textbf{e}			(k) 		= \textbf{y}(k) - \hat{\textbf{y}}(k|k-1)							\label{eq:Kalman_upd_inno}			\\
	&\textbf{S}			(k) 		= \textbf{C}\textbf{P}(k|k-1)\textbf{C}^T + \textbf{R}				\label{eq:Kalman_upd_inno_cov}		\\
	&\textbf{K}			(k) 		= \textbf{P}(k|k-1)\textbf{C}^T\textbf{S}^{-1}(k)					\label{eq:Kalman_upd_kalman_gain}	\\
	&\hat{\textbf{x}}	(k|k) 		= \hat{\textbf{x}}(k|k-1) + \textbf{K}(k)\textbf{e}(k) 				\label{eq:Kalman_upd_est_state}		\\
	&\textbf{P}			(k|k) 		= (\textbf{I} - \textbf{K}(k)\textbf{C})\textbf{P}(k|k-1)			\label{eq:Kalman_upd_est_cov}
\end{align}

where 

\begin{align*}
	&\text{$\textbf{e}		(k)$ 		is the innovation term for time index 					$ k $													}	\\[-1em]
	&\text{$\textbf{y}		(k)$ 		is the observed output at time index 					$ k $													}	\\[-1em]
	&\text{$\textbf{S}		(k)$ 		is the innovation covariance for time index 			$ k $											}	\\[-1em]
	&\text{$\textbf{R}$ 				is the observation noise covariance																}	\\[-1em]
	&\text{$\textbf{K}		(k)$ 		is the Kalman gain for time index 						$ k $													}	\\[-1em]
	&\text{$\hat{\textbf{x}}(k|k)$ 	is the estimate of the state vector at time index 			$ k $ given all 	$ k$ samples	}	\\[-1em]
	&\text{$\textbf{P}		(k|k)$ 	is the updated estimate covariance matrix at time index 	$ k $ given all 	$ k$ samples		}	\\[-1em]
	&\text{$\textbf{I}$ 				is the identity matrix																				}	\\[-1em]			
\end{align*}

All the equations in the update stage estimates the posteriori estimate of the $ k $ sample based on all $ k $ samples and the model knowledge. A few more remarks to give some intuitions to what the equations actually represent. The estimate covariance and innovation covariance can respectively be represented as 

\begin{align}
	&\textbf{P}(k|k) 	= \text{cov}(\textbf{x}(k)-	\hat{\textbf{x}}(k|k))	\\
	&\textbf{P}(k|k-1) 	= \text{cov}(\textbf{x}(k)-	\hat{\textbf{x}}(k|k-1)) 		\\
	&\textbf{S}(k) 		= \text{cov}(\textbf{e}(k)) 
\end{align}

\textbf{Compact form}\\
Now collecting some of the above equations gives the more compact form of the Kalman equations.
Eq.'s \ref{eq:Kalman_pred_state} and \ref{eq:Kalman_pred_cov} remains in the same form, but omitting $ \textbf{Bu}(k-1) $ yields \cref{eq:Kalman_pred_state_compact} and \cref{eq:Kalman_pred_cov_compact}. Eq.'s \ref{eq:Kalman_upd_inno_cov}, \ref{eq:Kalman_upd_kalman_gain} are combined yielding \cref{eq:Kalman_upd_kalman_gain_compact}. Eq. \ref{eq:Kalman_pred_output} is combined with \cref{eq:Kalman_upd_inno} and \cref{eq:Kalman_upd_est_state} yielding \cref{eq:Kalman_upd_est_state_compact}, while \cref{eq:Kalman_upd_est_cov} remains the same. All these combined presents the kalman equations as in \cite{Bozic1994}\\
\textbf{Prediction}

\begin{align}
	&\hat{\textbf{x}}	(k|k-1) = \textbf{A} \hat{\textbf{x}}	(k-1|k-1) 		\label{eq:Kalman_pred_state_compact} 	\\
	&\textbf{P}			(k|k-1) = \textbf{A}\textbf{P}			(k-1|k-1)\textbf{A}^T+\textbf{Q} 				\label{eq:Kalman_pred_cov_compact} 		
\end{align}

\textbf{Update}
\begin{align}
	&\textbf{K}			(k) 		= \textbf{P}				(k|k-1)\textbf{C}^T(\textbf{C}\textbf{P}	(k|k-1)	\textbf{C}^T + \textbf{R})^{-1}										\label{eq:Kalman_upd_kalman_gain_compact} \\
	&\hat{\textbf{x}}	(k|k) 	= \hat{\textbf{x}}			(k|k-1) + \textbf{K}						(k)	(\textbf{y}		(k) - \textbf{C}\hat{\textbf{x}}		(k|k-1)) 	\label{eq:Kalman_upd_est_state_compact} \\
	&\textbf{P}			(k|k) 	= (\textbf{I} - \textbf{K}	(k)\textbf{C})\textbf{P}					(k|k-1)																		\label{eq:Kalman_upd_est_cov_compact}
\end{align}


\subsubsection{Steady-state Kalman filter}
If we assume that our system is time invariant, which we have done so far, the estimate Covariance and Kalman gain can be calculated analytically as they don't depend on measurements and because A and C are constant. As such, the Kalman filter itself becomes a LTI filter. The steady state estimate covariance prediction can be found by solving its Algebraic Riccati equation. We will attempt to derive this.\\

%\begin{align}
%	&\textbf{P} = \textbf{APA}^T - \textbf{APC}^T(\textbf{CPC}^T+\textbf{R})^{-1} \textbf{CPA}^T+ \textbf{Q} \\
%	&\textbf{K} = \textbf{APC}^T(\textbf{CPC}^T+\textbf{R})^{-1}
%\end{align}

In steady-state where $k \rightarrow \infty$ we can assume that $\textbf{P}(k|k) = \textbf{P}(k-1|k-1) $. 

The predicted MSE, the MSE and the Kalman Gain approach a constant value:

\begin{equation}
	\textbf{P}(k|k-1) =  \textbf{P}_p(\infty), \textbf{P}(k|k) = \textbf{P}(k-1|k-1) = \textbf{P}(\infty), \textbf{K}(k) = \textbf{K}(\infty)
\end{equation}

Equations \cref{eq:Kalman_upd_kalman_gain_compact}, \cref{eq:Kalman_upd_est_state_compact} and \cref{eq:Kalman_upd_est_cov_compact} are rewritten with their steady state counterparts:
\begin{align}
	& \textbf{K}(\infty) = \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p	(\infty) \textbf{C}^T + \textbf{R})^{-1} \label{eq:ss_kalman_kalman_gain} \\
	& \textbf{P}(\infty) = (\textbf{I} - \textbf{K}(\infty) \textbf{C}) \textbf{P}_p(\infty) \label{eq:ss_kalman_mse} \\
	& \textbf{P}_p(\infty) = \textbf{A}\textbf{P}(\infty)A^T + Q \label{eq:ss_kalman_mse_pred}
\end{align}

We go on the hunt for the damened Riccati equation of $\textbf{P}_p(\infty)$:

\begin{equation} \label{eq:ss_kalman_udledning}
	\begin{split}
		\textbf{P}_p(\infty) & = \textbf{A} (I-\textbf{K}(\infty)C) \textbf{P}_p(\infty) \textbf{A}^T + \textbf{Q}\\
		& = \textbf{A} \textbf{P}_p(\infty) \textbf{A}^T - \textbf{A} \textbf{K}(\infty) \textbf{C} \textbf{P}_p(\infty)  \textbf{A}^T + \textbf{Q}\\
		& = \textbf{A} \textbf{P}_p(\infty) \textbf{A}^T - \textbf{A} \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p(\infty) \textbf{C}^T + \textbf{R})^{-1} \textbf{C} \textbf{P}_p(\infty) \textbf{A}^T + \textbf{Q}\\
		& = \textbf{A} (\textbf{P}_p(\infty) - \textbf{P}_p(\infty) \textbf{C}^T (\textbf{C} \textbf{P}_p(\infty) \textbf{C}^T + \textbf{R})^{-1} \textbf{C} \textbf{P}_p(\infty)) \textbf{A}^T + \textbf{Q}\\
		& = \textbf{A} (\textbf{P}_p(\infty)^{-1} + \textbf{C}^T \textbf{R}^{-1} \textbf{C})^{-1} \textbf{A}^T + \textbf{Q}\\
	\end{split}
\end{equation}

The Riccati equation found in \cref{eq:ss_kalman_udledning} can be solved for $\textbf{P}_p(\infty)$ numerically and substitued into \cref{eq:ss_kalman_kalman_gain} yielding  \textit{The Steady State Kalman Gain} which is then substitued into \cref{eq:ss_kalman_mse} to obtain \textit{Steady-State Estimate Covariance}.

\subsubsection{Implementation of Kalman filter}
For our case, the disturbance model has $ \textbf{u}(k)=0 $ meaning that our implementation can disregard the $ \textbf{B}\textbf{u}(k) $ in \cref{eq:Kalman_pred_state_compact}.


The steady state Covariance matrix P and kalman Gain is ... (asymtotic kalman gain and P)
Constant Q and R, there will be an asymptotic solution - Riccati equations. 

\subsubsection{Assumptions about noise}
Describe covariance matrices Q and R\\


